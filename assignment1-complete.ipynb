{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b7f610d",
   "metadata": {},
   "source": [
    "## Assignment 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5251020e",
   "metadata": {},
   "source": [
    "Shabbir Bhalamwala- 301383831 [corpus collection, loading files and common functions, count length, lexical diversity and longest sentence code write up]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038168a7",
   "metadata": {},
   "source": [
    "Muhammad -301420568 [ top collocations, top ten words with vowels and stemmed version of the longest sentence code write up]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af076864",
   "metadata": {},
   "source": [
    "Article 4 - Gell, A. (n.d.). 'that's not all!' Kevin Trudeau, the world's greatest salesman, makes one last pitch. Business Insider. Retrieved February 9, 2023, from https://www.businessinsider.com/kevin-trudeau-interview-2014-12 \n",
    "\n",
    "Article 2 -Bowden, M. (2019, July 9). We hired the author of 'black hawk down' and an illustrator from 'archer' to adapt the Mueller Report so you'll actually read it. Insider. Retrieved February 9, 2023, from https://www.insider.com/mueller-report-rewritten-trump-russia-mark-bowden-archer-2019-7  \n",
    "\n",
    "Book 1- WEST, DOUGLAS.B. (2005). Introduction to graph theory solution manual.Retrieved February 9, 2023, from https://bayanbox.ir/view/2051120523710881534/Introduction-to-graph-theory-solution-manual.pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b8220a",
   "metadata": {},
   "source": [
    "## Load Files and Common Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3ff0ccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.collocations import *\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "corpus_root = \"./corpus\"\n",
    "data = PlaintextCorpusReader(corpus_root, '.*', encoding = \"utf8\")\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    return str(len(set(text))/len(text))\n",
    "\n",
    "def longest_sentence(data, fileid):\n",
    "    sentences = data.sents(fileid)\n",
    "    max_len = 0\n",
    "    result = 0\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) > max_len:\n",
    "            max_len = len(sentence)\n",
    "            result = sentence\n",
    "            \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e40707",
   "metadata": {},
   "source": [
    "## 1- Count Length and lexical diversity for each files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4c131537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['article2.txt', 'article4.txt', 'book1.txt']\n",
      "article2.txt Length 13163 Lexical_diversity 0.19630783256096634\n",
      "\n",
      "\n",
      "article4.txt Length 15116 Lexical_diversity 0.21215930140248743\n",
      "\n",
      "\n",
      "book1.txt Length 356356 Lexical_diversity 0.018571316324125314\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data.fileids())\n",
    "for fileid in data.fileids():\n",
    "    num_words = len(data.words(fileid))\n",
    "    lexical_div = lexical_diversity(data.words(fileid))\n",
    "    print(fileid, \"Length\",num_words, \"Lexical_diversity\", lexical_div)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59823a2d",
   "metadata": {},
   "source": [
    "## 3- Longest Sentence in each files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8aa22b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article2.txt\n",
      "Length : 94 Sentence:  ['He', 'accused', 'him', 'of', 'having', 'sought', 'the', 'FBI', 'directorship', 'himself', '(', 'false', ',', 'he', 'had', 'been', 'invited', 'to', 'the', 'White', 'House', 'to', 'consult', 'on', 'finding', 'a', 'new', 'director', ');', 'of', 'fighting', 'with', 'the', 'Trump', 'Organization', 'over', 'his', 'departure', 'from', 'one', 'of', 'its', 'golf', 'clubs', '(', 'false', ',', 'correspondence', 'about', 'the', 'matter', 'had', 'been', 'routine', 'and', 'not', 'contentious', ');', 'and', 'of', 'having', 'conflicts', 'of', 'interest', 'because', 'members', 'of', 'his', 'law', 'firm', 'had', 'once', 'represented', 'members', 'of', 'Trump', \"'\", 's', 'family', '(', 'Mueller', 'himself', 'had', 'not', ',', 'and', 'it', \"'\", 's', 'a', 'large', 'law', 'firm', ').']\n",
      "\n",
      "\n",
      "article4.txt\n",
      "Length : 91 Sentence:  ['There', 'was', 'his', 'own', 'Mega', 'Memory', 'training', 'program', ';', 'the', 'Sable', 'Hair', '-', 'Farming', 'System', '(', 'which', 'he', 'promised', 'would', '\"', 'end', 'hair', 'loss', 'in', 'the', 'human', 'race', '\");', 'Dr', '.', 'Callahan', \"'\", 's', 'Addiction', '-', 'Breaking', 'System', '(', 'which', 'he', 'said', 'could', 'break', 'the', 'user', 'of', 'any', 'addiction', 'in', '60', 'seconds', '\"', 'virtually', '100', 'percent', 'of', 'the', 'time', '\");', 'Howard', 'Berg', \"'\", 's', 'Mega', 'Reading', 'speed', '-', 'reading', 'program', ',', 'the', 'Perfect', 'Lift', 'Non', '-', 'Surgical', 'Face', 'Lift', ',', 'and', 'Eden', \"'\", 's', 'Secret', 'Nature', \"'\", 's', 'Purifying', 'Product', '.']\n",
      "\n",
      "\n",
      "book1.txt\n",
      "Length : 232 Sentence:  ['Let', 'x', 'be', 'a', 'vertex', 'mapped', 'to', 'a', 'vertex', 'of', 'weight', 'j', 'in', 'Q', 'l', '.', 'For', 'j', '≤', '1', ',', 'we', 'have', 'already', 'argued', 'that', 'x', 'differs', 'from', 'v', 'in', 'j', 'positions', 'of', 'S', '.', 'For', 'j', '≥', '2', ',', 'let', 'y', 'and', 'z', 'be', 'two', 'neighbors', 'of', 'x', 'whose', 'images', 'under', 'f', 'have', 'weight', 'j', '−', '1', 'in', 'Q', 'l', '.', 'By', 'the', 'induction', 'hypothesis', ',', 'y', 'and', 'z', 'differ', 'from', 'v', 'in', 'j', 'positions', 'of', 'S', '.', 'Since', 'f', '(', 'y', ')', 'and', 'f', '(', 'z', ')', 'differ', 'in', 'two', 'places', ',', 'they', 'have', 'two', 'common', 'neighbors', 'in', 'Q', 'l', ',', 'which', 'are', 'x', 'and', 'another', 'vertex', 'w', '.', 'Since', 'w', 'has', 'weight', 'j', '−', '2', ',', 'the', 'induction', 'hypothesis', 'yields', 'that', 'w', 'differs', 'from', 'v', 'in', 'j', '−', '1', 'positions', 'of', 'S', '.', 'Since', 'the', 'images', 'of', 'x', ',', 'y', ',', 'z', ',', 'w', 'induce', 'a', '4', '-', 'cycle', 'in', 'Q', 'l', ',', 'also', 'x', ',', 'y', ',', 'z', ',', 'w', 'induce', 'a', '4', '-', 'cycle', 'in', 'H', '.', 'The', 'only', '4', '-', 'cycle', 'in', 'Q', 'k', 'that', 'contains', 'all', 'of', 'y', ',', 'z', ',', 'w', 'adds', 'the', 'vertex', 'that', 'differs', 'from', 'v', 'in', 'the', 'j', '−', '2', 'positions', 'of', 'S', 'where', 'w', 'differs', ',', 'plus', 'the', 'two', 'positions', 'where', 'y', 'and', 'z', 'differ', 'from', 'w', '.', 'This', 'completes', 'the', 'proof', 'that', 'x', 'has', 'the', 'desired', 'property', '.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fileid in data.fileids():\n",
    "    long_sentence = longest_sentence(data, fileid)\n",
    "    print(fileid)\n",
    "    print(\"Length :\", len(long_sentence), \"Sentence: \" ,long_sentence)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcb51dc",
   "metadata": {},
   "source": [
    "## 4- Top Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "655f3515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article2.txt\n",
      "White House; Report Hed; attorney general; Chad Hurd; Business\n",
      "Insider; Mueller Report; United States; Justice Department; Attorney\n",
      "General; FBI director; Hope Hicks; Logan Act; New York; Washington\n",
      "Post; special counsel; Donald Trump; :// muellerbusinessinsider; https\n",
      "://; nothing wrong; President Donald\n",
      "None\n",
      "\n",
      "\n",
      "article4.txt\n",
      "Van Liew; Loss Cure; Natural Cures; Weight Loss; Global Information;\n",
      "never heard; Information Network; Business Insider; Fred Van; FPC\n",
      "Montgomery; New York; Kristine Dorow; Abe Husein; Peter Wink; bowling\n",
      "alley; high school; motivational speaker; York Times; Nataliya\n",
      "Babenko; Cures Revealed\n",
      "None\n",
      "\n",
      "\n",
      "book1.txt\n",
      "induction hypothesis; may assume; spanning tree; independent set;\n",
      "exactly one; Additional Topics; Petersen graph; perfect matching;\n",
      "spanning trees; simple graph; partite set; lower bound; Fundamental\n",
      "Concepts; every vertex; partite sets; use induction; odd cycle;\n",
      "bipartite graph; Eulerian circuit; one vertex\n",
      "None\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fileid in data.fileids():\n",
    "    text = nltk.Text(data.words(fileid))\n",
    "    print(fileid)\n",
    "    print(text.collocations())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73a31bb",
   "metadata": {},
   "source": [
    "## 5- Top ten words with vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "60bf39a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article2.txt\n",
      "[('a', 260), ('of', 212), ('and', 197), ('in', 120), ('it', 104), ('on', 81), ('as', 43), ('about', 43), ('investigation', 40), ('asked', 36)]\n",
      "\n",
      "\n",
      "article4.txt\n",
      "[('a', 374), ('and', 284), ('of', 251), ('in', 228), ('it', 115), ('on', 71), ('as', 63), ('is', 58), ('an', 53), ('about', 47)]\n",
      "\n",
      "\n",
      "book1.txt\n",
      "[('of', 8758), ('a', 7175), ('is', 5893), ('and', 5695), ('in', 4107), ('edges', 1862), ('at', 1677), ('are', 1647), ('edge', 1593), ('e', 1589)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fileid in data.fileids():\n",
    "    text = data.words(fileid)\n",
    "    text2 = []\n",
    "    for word in text:\n",
    "        if word[0] in \"aeiou\":\n",
    "            text2.append(word)\n",
    "        else:\n",
    "            continue\n",
    "    fdist = nltk.FreqDist(text2)\n",
    "    result = fdist.most_common(10)\n",
    "    print(fileid)\n",
    "    print(result)\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ac6f1d",
   "metadata": {},
   "source": [
    "## 6- Stemmed version of the longest Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "27ba1832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article2.txt\n",
      "He  :  he\n",
      "accused  :  accus\n",
      "him  :  him\n",
      "of  :  of\n",
      "having  :  have\n",
      "sought  :  sought\n",
      "the  :  the\n",
      "FBI  :  fbi\n",
      "directorship  :  directorship\n",
      "himself  :  himself\n",
      "(  :  (\n",
      "false  :  fals\n",
      ",  :  ,\n",
      "he  :  he\n",
      "had  :  had\n",
      "been  :  been\n",
      "invited  :  invit\n",
      "to  :  to\n",
      "the  :  the\n",
      "White  :  white\n",
      "House  :  hous\n",
      "to  :  to\n",
      "consult  :  consult\n",
      "on  :  on\n",
      "finding  :  find\n",
      "a  :  a\n",
      "new  :  new\n",
      "director  :  director\n",
      ");  :  );\n",
      "of  :  of\n",
      "fighting  :  fight\n",
      "with  :  with\n",
      "the  :  the\n",
      "Trump  :  trump\n",
      "Organization  :  organ\n",
      "over  :  over\n",
      "his  :  hi\n",
      "departure  :  departur\n",
      "from  :  from\n",
      "one  :  one\n",
      "of  :  of\n",
      "its  :  it\n",
      "golf  :  golf\n",
      "clubs  :  club\n",
      "(  :  (\n",
      "false  :  fals\n",
      ",  :  ,\n",
      "correspondence  :  correspond\n",
      "about  :  about\n",
      "the  :  the\n",
      "matter  :  matter\n",
      "had  :  had\n",
      "been  :  been\n",
      "routine  :  routin\n",
      "and  :  and\n",
      "not  :  not\n",
      "contentious  :  contenti\n",
      ");  :  );\n",
      "and  :  and\n",
      "of  :  of\n",
      "having  :  have\n",
      "conflicts  :  conflict\n",
      "of  :  of\n",
      "interest  :  interest\n",
      "because  :  becaus\n",
      "members  :  member\n",
      "of  :  of\n",
      "his  :  hi\n",
      "law  :  law\n",
      "firm  :  firm\n",
      "had  :  had\n",
      "once  :  onc\n",
      "represented  :  repres\n",
      "members  :  member\n",
      "of  :  of\n",
      "Trump  :  trump\n",
      "'  :  '\n",
      "s  :  s\n",
      "family  :  famili\n",
      "(  :  (\n",
      "Mueller  :  mueller\n",
      "himself  :  himself\n",
      "had  :  had\n",
      "not  :  not\n",
      ",  :  ,\n",
      "and  :  and\n",
      "it  :  it\n",
      "'  :  '\n",
      "s  :  s\n",
      "a  :  a\n",
      "large  :  larg\n",
      "law  :  law\n",
      "firm  :  firm\n",
      ").  :  ).\n",
      "\n",
      "\n",
      "article4.txt\n",
      "There  :  there\n",
      "was  :  wa\n",
      "his  :  hi\n",
      "own  :  own\n",
      "Mega  :  mega\n",
      "Memory  :  memori\n",
      "training  :  train\n",
      "program  :  program\n",
      ";  :  ;\n",
      "the  :  the\n",
      "Sable  :  sabl\n",
      "Hair  :  hair\n",
      "-  :  -\n",
      "Farming  :  farm\n",
      "System  :  system\n",
      "(  :  (\n",
      "which  :  which\n",
      "he  :  he\n",
      "promised  :  promis\n",
      "would  :  would\n",
      "\"  :  \"\n",
      "end  :  end\n",
      "hair  :  hair\n",
      "loss  :  loss\n",
      "in  :  in\n",
      "the  :  the\n",
      "human  :  human\n",
      "race  :  race\n",
      "\");  :  \");\n",
      "Dr  :  dr\n",
      ".  :  .\n",
      "Callahan  :  callahan\n",
      "'  :  '\n",
      "s  :  s\n",
      "Addiction  :  addict\n",
      "-  :  -\n",
      "Breaking  :  break\n",
      "System  :  system\n",
      "(  :  (\n",
      "which  :  which\n",
      "he  :  he\n",
      "said  :  said\n",
      "could  :  could\n",
      "break  :  break\n",
      "the  :  the\n",
      "user  :  user\n",
      "of  :  of\n",
      "any  :  ani\n",
      "addiction  :  addict\n",
      "in  :  in\n",
      "60  :  60\n",
      "seconds  :  second\n",
      "\"  :  \"\n",
      "virtually  :  virtual\n",
      "100  :  100\n",
      "percent  :  percent\n",
      "of  :  of\n",
      "the  :  the\n",
      "time  :  time\n",
      "\");  :  \");\n",
      "Howard  :  howard\n",
      "Berg  :  berg\n",
      "'  :  '\n",
      "s  :  s\n",
      "Mega  :  mega\n",
      "Reading  :  read\n",
      "speed  :  speed\n",
      "-  :  -\n",
      "reading  :  read\n",
      "program  :  program\n",
      ",  :  ,\n",
      "the  :  the\n",
      "Perfect  :  perfect\n",
      "Lift  :  lift\n",
      "Non  :  non\n",
      "-  :  -\n",
      "Surgical  :  surgic\n",
      "Face  :  face\n",
      "Lift  :  lift\n",
      ",  :  ,\n",
      "and  :  and\n",
      "Eden  :  eden\n",
      "'  :  '\n",
      "s  :  s\n",
      "Secret  :  secret\n",
      "Nature  :  natur\n",
      "'  :  '\n",
      "s  :  s\n",
      "Purifying  :  purifi\n",
      "Product  :  product\n",
      ".  :  .\n",
      "\n",
      "\n",
      "book1.txt\n",
      "Let  :  let\n",
      "x  :  x\n",
      "be  :  be\n",
      "a  :  a\n",
      "vertex  :  vertex\n",
      "mapped  :  map\n",
      "to  :  to\n",
      "a  :  a\n",
      "vertex  :  vertex\n",
      "of  :  of\n",
      "weight  :  weight\n",
      "j  :  j\n",
      "in  :  in\n",
      "Q  :  q\n",
      "l  :  l\n",
      ".  :  .\n",
      "For  :  for\n",
      "j  :  j\n",
      "≤  :  ≤\n",
      "1  :  1\n",
      ",  :  ,\n",
      "we  :  we\n",
      "have  :  have\n",
      "already  :  alreadi\n",
      "argued  :  argu\n",
      "that  :  that\n",
      "x  :  x\n",
      "differs  :  differ\n",
      "from  :  from\n",
      "v  :  v\n",
      "in  :  in\n",
      "j  :  j\n",
      "positions  :  posit\n",
      "of  :  of\n",
      "S  :  s\n",
      ".  :  .\n",
      "For  :  for\n",
      "j  :  j\n",
      "≥  :  ≥\n",
      "2  :  2\n",
      ",  :  ,\n",
      "let  :  let\n",
      "y  :  y\n",
      "and  :  and\n",
      "z  :  z\n",
      "be  :  be\n",
      "two  :  two\n",
      "neighbors  :  neighbor\n",
      "of  :  of\n",
      "x  :  x\n",
      "whose  :  whose\n",
      "images  :  imag\n",
      "under  :  under\n",
      "f  :  f\n",
      "have  :  have\n",
      "weight  :  weight\n",
      "j  :  j\n",
      "−  :  −\n",
      "1  :  1\n",
      "in  :  in\n",
      "Q  :  q\n",
      "l  :  l\n",
      ".  :  .\n",
      "By  :  by\n",
      "the  :  the\n",
      "induction  :  induct\n",
      "hypothesis  :  hypothesi\n",
      ",  :  ,\n",
      "y  :  y\n",
      "and  :  and\n",
      "z  :  z\n",
      "differ  :  differ\n",
      "from  :  from\n",
      "v  :  v\n",
      "in  :  in\n",
      "j  :  j\n",
      "positions  :  posit\n",
      "of  :  of\n",
      "S  :  s\n",
      ".  :  .\n",
      "Since  :  sinc\n",
      "f  :  f\n",
      "(  :  (\n",
      "y  :  y\n",
      ")  :  )\n",
      "and  :  and\n",
      "f  :  f\n",
      "(  :  (\n",
      "z  :  z\n",
      ")  :  )\n",
      "differ  :  differ\n",
      "in  :  in\n",
      "two  :  two\n",
      "places  :  place\n",
      ",  :  ,\n",
      "they  :  they\n",
      "have  :  have\n",
      "two  :  two\n",
      "common  :  common\n",
      "neighbors  :  neighbor\n",
      "in  :  in\n",
      "Q  :  q\n",
      "l  :  l\n",
      ",  :  ,\n",
      "which  :  which\n",
      "are  :  are\n",
      "x  :  x\n",
      "and  :  and\n",
      "another  :  anoth\n",
      "vertex  :  vertex\n",
      "w  :  w\n",
      ".  :  .\n",
      "Since  :  sinc\n",
      "w  :  w\n",
      "has  :  ha\n",
      "weight  :  weight\n",
      "j  :  j\n",
      "−  :  −\n",
      "2  :  2\n",
      ",  :  ,\n",
      "the  :  the\n",
      "induction  :  induct\n",
      "hypothesis  :  hypothesi\n",
      "yields  :  yield\n",
      "that  :  that\n",
      "w  :  w\n",
      "differs  :  differ\n",
      "from  :  from\n",
      "v  :  v\n",
      "in  :  in\n",
      "j  :  j\n",
      "−  :  −\n",
      "1  :  1\n",
      "positions  :  posit\n",
      "of  :  of\n",
      "S  :  s\n",
      ".  :  .\n",
      "Since  :  sinc\n",
      "the  :  the\n",
      "images  :  imag\n",
      "of  :  of\n",
      "x  :  x\n",
      ",  :  ,\n",
      "y  :  y\n",
      ",  :  ,\n",
      "z  :  z\n",
      ",  :  ,\n",
      "w  :  w\n",
      "induce  :  induc\n",
      "a  :  a\n",
      "4  :  4\n",
      "-  :  -\n",
      "cycle  :  cycl\n",
      "in  :  in\n",
      "Q  :  q\n",
      "l  :  l\n",
      ",  :  ,\n",
      "also  :  also\n",
      "x  :  x\n",
      ",  :  ,\n",
      "y  :  y\n",
      ",  :  ,\n",
      "z  :  z\n",
      ",  :  ,\n",
      "w  :  w\n",
      "induce  :  induc\n",
      "a  :  a\n",
      "4  :  4\n",
      "-  :  -\n",
      "cycle  :  cycl\n",
      "in  :  in\n",
      "H  :  h\n",
      ".  :  .\n",
      "The  :  the\n",
      "only  :  onli\n",
      "4  :  4\n",
      "-  :  -\n",
      "cycle  :  cycl\n",
      "in  :  in\n",
      "Q  :  q\n",
      "k  :  k\n",
      "that  :  that\n",
      "contains  :  contain\n",
      "all  :  all\n",
      "of  :  of\n",
      "y  :  y\n",
      ",  :  ,\n",
      "z  :  z\n",
      ",  :  ,\n",
      "w  :  w\n",
      "adds  :  add\n",
      "the  :  the\n",
      "vertex  :  vertex\n",
      "that  :  that\n",
      "differs  :  differ\n",
      "from  :  from\n",
      "v  :  v\n",
      "in  :  in\n",
      "the  :  the\n",
      "j  :  j\n",
      "−  :  −\n",
      "2  :  2\n",
      "positions  :  posit\n",
      "of  :  of\n",
      "S  :  s\n",
      "where  :  where\n",
      "w  :  w\n",
      "differs  :  differ\n",
      ",  :  ,\n",
      "plus  :  plu\n",
      "the  :  the\n",
      "two  :  two\n",
      "positions  :  posit\n",
      "where  :  where\n",
      "y  :  y\n",
      "and  :  and\n",
      "z  :  z\n",
      "differ  :  differ\n",
      "from  :  from\n",
      "w  :  w\n",
      ".  :  .\n",
      "This  :  thi\n",
      "completes  :  complet\n",
      "the  :  the\n",
      "proof  :  proof\n",
      "that  :  that\n",
      "x  :  x\n",
      "has  :  ha\n",
      "the  :  the\n",
      "desired  :  desir\n",
      "property  :  properti\n",
      ".  :  .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fileid in data.fileids():\n",
    "    long_sentence = longest_sentence(data, fileid)\n",
    "    print(fileid)\n",
    "    for w in long_sentence:\n",
    "        print(w, \" : \", PorterStemmer().stem(w))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd31523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
